# -*- coding: utf-8 -*-
"""doMusicAndSpeechDetection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/satvik-venkatesh/audio-seg-data-synth/blob/main/models/doMusicAndSpeechDetection.ipynb
"""

import argparse
import numpy as np
import librosa
import math
from musicspeech_class import MusicSpeechClass

class MusicSpeechController():
  
  def __init__(self, params):
    
    self.model = MusicSpeechClass()
    self.params = params
  
  def frames_to_time(self, f, sr = 22050.0, hop_size = 220):
    return f * hop_size / sr

  def get_log_melspectrogram(self, audio, sr = 22050, hop_length = 220, n_fft = 1024, n_mels = 80, fmin = 64, fmax = 8000):
    """Return the log-scaled Mel bands of an audio signal."""
    bands = librosa.feature.melspectrogram(
        y=audio, sr=sr, hop_length=hop_length, n_fft=n_fft, n_mels=n_mels, fmin=fmin, fmax=fmax, dtype=np.float32)
    
    return librosa.core.power_to_db(bands, amin=1e-7)

  def smooth_output(self, output, min_speech=1.3, min_music=3.4, max_silence_speech=0.4, max_silence_music=0.6):
    """ This function was adapted from https://github.com/qlemaire22/speech-music-detection """

    duration_frame = 220 / 22050
    n_frame = output.shape[1]

    start_music = -1000
    start_speech = -1000

    for i in range(n_frame):
        if output[0, i] == 1:
            if i - start_speech > 1:
                if (i - start_speech) * duration_frame <= max_silence_speech:
                    output[0, start_speech:i] = 1

            start_speech = i

        if output[1, i] == 1:
            if i - start_music > 1:
                if (i - start_music) * duration_frame <= max_silence_music:
                    output[1, start_music:i] = 1

            start_music = i

    start_music = -1000
    start_speech = -1000

    for i in range(n_frame):
        if i != n_frame - 1:
            if output[0, i] == 0:
                if i - start_speech > 1:
                    if (i - start_speech) * duration_frame <= min_speech:
                        output[0, start_speech:i] = 0

                start_speech = i

            if output[1, i] == 0:
                if i - start_music > 1:
                    if (i - start_music) * duration_frame <= min_music:
                        output[1, start_music:i] = 0

                start_music = i
        else:
            if i - start_speech > 1:
                if (i - start_speech) * duration_frame <= min_speech:
                    output[0, start_speech:i + 1] = 0

            if i - start_music > 1:
                if (i - start_music) * duration_frame <= min_music:
                    output[1, start_music:i + 1] = 0

    return output

  def preds_to_se(self, p, audio_clip_length = 8.0):
    """
    This function converts the predictions made by the neural network into a readable format.
    """
    start_speech = -100
    start_music = -100
    stop_speech = -100
    stop_music = -100

    audio_events = []

    n_frames = p.shape[0]

    if p[0, 0] == 1:
      start_speech = 0
    
    if p[0, 1] == 1:
      start_music = 0

    for i in range(n_frames - 1):
      if p[i, 0] == 0 and p[i + 1, 0] == 1:
        start_speech = i + 1

      elif p[i, 0] == 1 and p[i + 1, 0] == 0:
        stop_speech = i
        start_time = self.frames_to_time(start_speech)
        stop_time = self.frames_to_time(stop_speech)
        audio_events.append((start_time, stop_time, "speech"))
        start_speech = -100
        stop_speech = -100

      if p[i, 1] == 0 and p[i + 1, 1] == 1:
        start_music = i + 1
      elif p[i, 1] == 1 and p[i + 1, 1] == 0:
        stop_music = i
        start_time = self.frames_to_time(start_music)
        stop_time = self.frames_to_time(stop_music)      
        audio_events.append((start_time, stop_time, "music"))
        start_music = -100
        stop_music = -100

    if start_speech != -100:
      start_time = self.frames_to_time(start_speech)
      stop_time = audio_clip_length
      audio_events.append((start_time, stop_time, "speech"))
      start_speech = -100
      stop_speech = -100

    if start_music != -100:
      start_time = self.frames_to_time(start_music)
      stop_time = audio_clip_length
      audio_events.append((start_time, stop_time, "music"))
      start_music = -100
      stop_music = -100

    audio_events.sort(key = lambda x: x[0]) 
    return audio_events

  def mk_preds_fa(self, in_signal, 
                  hop_size = 6.0, 
                  discard = 1.0, 
                  win_length = 8.0, 
                  sampling_rate = 22050):
    """
    Make predictions for full audio.
    """
    # Pad the input signal if it is shorter than 8 s.

    if in_signal.shape[0] < int(8.0 * sampling_rate):
      pad_signal = np.zeros((int(8.0 * sampling_rate)))
      pad_signal[:in_signal.shape[0]] = in_signal
      in_signal = np.copy(pad_signal)

    audio_clip_length_samples = in_signal.shape[0]
    print('audio_clip_length_samples is {}'.format(audio_clip_length_samples))

    hop_size_samples = 220 * 602 - 1
    win_length_samples = 220 * 802 - 1

    n_preds = int(math.ceil((audio_clip_length_samples - win_length_samples) / hop_size_samples)) + 1
    in_signal_pad = np.zeros((n_preds * hop_size_samples + 200 * 220))
    in_signal_pad[0:audio_clip_length_samples] = in_signal
    preds = np.zeros((n_preds, 802, 2))

    # Split the predictions into batches of size batch_size. 
    batch_size = self.params.batch_size
    n_batch = n_preds // self.params.batch_size

    for i in range(n_batch):
      mss_batch = np.zeros((batch_size, 802, 80))
      for j in range(batch_size):
        seg = in_signal_pad[(i * batch_size + j)* hop_size_samples:((i * batch_size + j) * hop_size_samples) + win_length_samples]
        seg = librosa.util.normalize(seg)
        mss = self.get_log_melspectrogram(seg)
        M = mss.T
        mss_batch[j, :, :] = M

      preds[i * batch_size:(i + 1) * batch_size, :, :] = (self.model.predict(mss_batch) >= (0.5, 0.5)).astype(float)

    if n_batch * batch_size < n_preds:
      i = n_batch
      mss_batch = np.zeros((n_preds - n_batch * batch_size, 802, 80))
      for j in range(n_preds - n_batch * batch_size):
        seg = in_signal_pad[(i * batch_size + j)* hop_size_samples:((i * batch_size + j) * hop_size_samples) + win_length_samples]
        seg = librosa.util.normalize(seg)
        mss = self.get_log_melspectrogram(seg)
        M = mss.T
        mss_batch[j, :, :] = M

      preds[i * batch_size:n_preds, :, :] = (self.model.predict(mss_batch) >= (0.5, 0.5)).astype(float)

    preds_mid = np.copy(preds[1:-1, 100:702, :])

    preds_mid_2 = preds_mid.reshape(-1, 2)

    if preds.shape[0] > 1:
      oa_preds = preds[0, 0:702, :] # oa stands for overall predictions
    else:
      oa_preds = preds[0, 0:802, :] # oa stands for overall predictions

    oa_preds = np.concatenate((oa_preds, preds_mid_2), axis = 0)

    if preds.shape[0] > 1:
      oa_preds = np.concatenate((oa_preds, preds[-1, 100:, :]), axis = 0)

    return oa_preds

  def predict(self, input_data, fs=None):

    if isinstance(input_data, str):
      input_data,fs = librosa.load(input_data, mono=True, sr=self.params.sample_rate)
    
    if fs != self.params.sample_rate:
      input_data = librosa.resample(input_data, orig_sr=fs, target_sr=self.params.sample_rate)
      
    oop = self.mk_preds_fa(input_data)

    p_smooth = self.smooth_output(oop.T, min_speech=1.3, min_music=3.4, max_silence_speech=0.4, max_silence_music=0.6)
    p_smooth = p_smooth.T
    see = self.preds_to_se(p_smooth, audio_clip_length=input_data.size/self.params.sample_rate)    
    
    return see
