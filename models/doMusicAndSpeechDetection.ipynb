{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/satvik-venkatesh/audio-seg-data-synth/blob/main/models/doMusicAndSpeechDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rOFJwr-6na0f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-04-10 07:11:12.580146: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-04-10 07:11:12.697060: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "2024-04-10 07:11:12.697080: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "2024-04-10 07:11:13.241417: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
            "2024-04-10 07:11:13.241478: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
            "2024-04-10 07:11:13.241485: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
          ]
        }
      ],
      "source": [
        "import soundfile as sf\n",
        "import argparse\n",
        "import numpy as np\n",
        "import librosa\n",
        "import tensorflow as tf\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = np.array([1,2,3])\n",
        "x.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-04-10 07:11:18.028344: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
            "2024-04-10 07:11:18.028367: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
            "2024-04-10 07:11:18.028386: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (condor2363): /proc/driver/nvidia/version does not exist\n",
            "2024-04-10 07:11:18.028590: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "model = tf.keras.models.load_model(\"model d-DS.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZ9J12x7ZHNu"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "vJEnwns-nduO"
      },
      "outputs": [],
      "source": [
        "def smooth_output(output, min_speech=1.3, min_music=3.4, max_silence_speech=0.4, max_silence_music=0.6):\n",
        "    duration_frame = 220 / 22050\n",
        "    n_frame = output.shape[1]\n",
        "\n",
        "    start_music = -1000\n",
        "    start_speech = -1000\n",
        "\n",
        "    for i in range(n_frame):\n",
        "        if output[0, i] == 1:\n",
        "            if i - start_speech > 1:\n",
        "                if (i - start_speech) * duration_frame <= max_silence_speech:\n",
        "                    output[0, start_speech:i] = 1\n",
        "\n",
        "            start_speech = i\n",
        "\n",
        "        if output[1, i] == 1:\n",
        "            if i - start_music > 1:\n",
        "                if (i - start_music) * duration_frame <= max_silence_music:\n",
        "                    output[1, start_music:i] = 1\n",
        "\n",
        "            start_music = i\n",
        "\n",
        "    start_music = -1000\n",
        "    start_speech = -1000\n",
        "\n",
        "    for i in range(n_frame):\n",
        "        if i != n_frame - 1:\n",
        "            if output[0, i] == 0:\n",
        "                if i - start_speech > 1:\n",
        "                    if (i - start_speech) * duration_frame <= min_speech:\n",
        "                        output[0, start_speech:i] = 0\n",
        "\n",
        "                start_speech = i\n",
        "\n",
        "            if output[1, i] == 0:\n",
        "                if i - start_music > 1:\n",
        "                    if (i - start_music) * duration_frame <= min_music:\n",
        "                        output[1, start_music:i] = 0\n",
        "\n",
        "                start_music = i\n",
        "        else:\n",
        "            if i - start_speech > 1:\n",
        "                if (i - start_speech) * duration_frame <= min_speech:\n",
        "                    output[0, start_speech:i + 1] = 0\n",
        "\n",
        "            if i - start_music > 1:\n",
        "                if (i - start_music) * duration_frame <= min_music:\n",
        "                    output[1, start_music:i + 1] = 0\n",
        "\n",
        "    return output\n",
        "\n",
        "\"\"\"\n",
        "This function converts the predictions made by the neural network into a format that is understood by sed_eval.\n",
        "\"\"\"\n",
        "\n",
        "def preds_to_se(p, audio_clip_length = 8.0):\n",
        "  start_speech = -100\n",
        "  start_music = -100\n",
        "  stop_speech = -100\n",
        "  stop_music = -100\n",
        "\n",
        "  audio_events = []\n",
        "\n",
        "  n_frames = p.shape[0]\n",
        "\n",
        "  if p[0, 0] == 1:\n",
        "    start_speech = 0\n",
        "  \n",
        "  if p[0, 1] == 1:\n",
        "    start_music = 0\n",
        "\n",
        "  for i in range(n_frames - 1):\n",
        "    if p[i, 0] == 0 and p[i + 1, 0] == 1:\n",
        "      start_speech = i + 1\n",
        "\n",
        "    elif p[i, 0] == 1 and p[i + 1, 0] == 0:\n",
        "      stop_speech = i\n",
        "      start_time = frames_to_time(start_speech)\n",
        "      stop_time = frames_to_time(stop_speech)\n",
        "      audio_events.append((start_time, stop_time, \"speech\"))\n",
        "      start_speech = -100\n",
        "      stop_speech = -100\n",
        "\n",
        "    if p[i, 1] == 0 and p[i + 1, 1] == 1:\n",
        "      start_music = i + 1\n",
        "    elif p[i, 1] == 1 and p[i + 1, 1] == 0:\n",
        "      stop_music = i\n",
        "      start_time = frames_to_time(start_music)\n",
        "      stop_time = frames_to_time(stop_music)      \n",
        "      audio_events.append((start_time, stop_time, \"music\"))\n",
        "      start_music = -100\n",
        "      stop_music = -100\n",
        "\n",
        "  if start_speech != -100:\n",
        "    start_time = frames_to_time(start_speech)\n",
        "    stop_time = audio_clip_length\n",
        "    audio_events.append((start_time, stop_time, \"speech\"))\n",
        "    start_speech = -100\n",
        "    stop_speech = -100\n",
        "\n",
        "  if start_music != -100:\n",
        "    start_time = frames_to_time(start_music)\n",
        "    stop_time = audio_clip_length\n",
        "    audio_events.append((start_time, stop_time, \"music\"))\n",
        "    start_music = -100\n",
        "    stop_music = -100\n",
        "\n",
        "  audio_events.sort(key = lambda x: x[0]) \n",
        "  return audio_events\n",
        "\n",
        "def frames_to_time(f, sr = 22050.0, hop_size = 220):\n",
        "  return f * hop_size / sr\n",
        "\n",
        "def get_log_melspectrogram(audio, sr = 22050, hop_length = 220, n_fft = 1024, n_mels = 128, fmin = 64, fmax = 8000):\n",
        "    \"\"\"Return the log-scaled Mel bands of an audio signal.\"\"\"\n",
        "    bands = librosa.feature.melspectrogram(\n",
        "        y=audio, sr=sr, hop_length=hop_length, n_fft=n_fft, n_mels=n_mels, fmin=fmin, fmax=fmax, dtype=np.float32)\n",
        "    return librosa.core.power_to_db(bands, amin=1e-7)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Make predictions for full audio.\n",
        "\"\"\"\n",
        "def mk_preds_fa(audio_path, hop_size = 6.0, discard = 1.0, win_length = 8.0, sampling_rate = 22050):\n",
        "  in_signal, in_sr = sf.read(audio_path)\n",
        "\n",
        "  # Convert to mono if needed.\n",
        "  if (in_signal.ndim > 1):\n",
        "    in_signal_mono = librosa.to_mono(in_signal)\n",
        "    in_signal = np.copy(in_signal_mono)\n",
        "  # Resample the audio file.\n",
        "  in_signal_22k = librosa.resample(in_signal, orig_sr=in_sr, target_sr=sampling_rate)\n",
        "  in_signal = np.copy(in_signal_22k)\n",
        "\n",
        "  audio_clip_length_samples = in_signal.shape[0]\n",
        "  print('audio_clip_length_samples is {}'.format(audio_clip_length_samples))\n",
        "\n",
        "  #hop_size_samples = int(hop_size * sampling_rate)\n",
        "  hop_size_samples = 220 * 602 - 1\n",
        "\n",
        "  #win_length_samples = int(win_length * sampling_rate)\n",
        "  win_length_samples = 220 * 802 - 1\n",
        "\n",
        "  n_preds = int(math.ceil((audio_clip_length_samples - win_length_samples) / hop_size_samples)) + 1\n",
        "\n",
        "  #print('n_preds is {}'.format(n_preds))\n",
        "\n",
        "  in_signal_pad = np.zeros((n_preds * hop_size_samples + 200 * 220))\n",
        "\n",
        "  #print('in_signal_pad.shape is {}'.format(in_signal_pad.shape))\n",
        "\n",
        "  in_signal_pad[0:audio_clip_length_samples] = in_signal\n",
        "\n",
        "  preds = np.zeros((n_preds, 802, 2))\n",
        "  mss_in = np.zeros((n_preds, 802, 128))\n",
        "\n",
        "  for i in range(n_preds):\n",
        "    seg = in_signal_pad[i * hop_size_samples:(i * hop_size_samples) + win_length_samples]\n",
        "    #print('seg.shape is {}'.format(seg.shape))\n",
        "    seg = librosa.util.normalize(seg)\n",
        "\n",
        "    mss = get_log_melspectrogram(seg)\n",
        "    M = mss.T\n",
        "    mss_in[i, :, :] = M\n",
        "\n",
        "  preds = (model.predict(mss_in) >= (0.5, 0.5)).astype(np.float32)\n",
        "\n",
        "  preds_mid = np.copy(preds[1:-1, 100:702, :])\n",
        "\n",
        "  preds_mid_2 = preds_mid.reshape(-1, 2)\n",
        "\n",
        "  oa_preds = preds[0, 0:702, :] # oa stands for overall predictions\n",
        "\n",
        "  oa_preds = np.concatenate((oa_preds, preds_mid_2), axis = 0)\n",
        "\n",
        "  oa_preds = np.concatenate((oa_preds, preds[-1, 100:, :]), axis = 0)\n",
        "\n",
        "  return oa_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "fdzW04oyk_AF"
      },
      "outputs": [],
      "source": [
        "# parser = argparse.ArgumentParser(description=\"Music and speech detection on a given audio and output as txt file\")\n",
        "# parser.add_argument('input_path', help='Input wav file path')\n",
        "# parser.add_argument('output_path', help=\"Output txt file path\")\n",
        "\n",
        "# args = parser.parse_args()\n",
        "\n",
        "from types import SimpleNamespace\n",
        "audio = \"example-1\"\n",
        "args = SimpleNamespace(\n",
        "    input_path=f\"/home/lennon/workspace/audio-seg-data-synth/Synthetic Radio Examples/{audio}.wav\",\n",
        "    output_path=f\"/home/lennon/workspace/audio-seg-data-synth/Synthetic Radio Examples/{audio}-result.txt\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "xA4ITBuORNSd"
      },
      "outputs": [],
      "source": [
        "test_audio = args.input_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "5mgghPfWoiCX",
        "outputId": "20a96105-e401-4bcd-dfc3-8ebd8d9a0024"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "audio_clip_length_samples is 176400\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"/home/lennon/miniconda3/envs/keras2/lib/python3.9/site-packages/keras/engine/training.py\", line 2137, in predict_function  *\n        return step_function(self, iterator)\n    File \"/home/lennon/miniconda3/envs/keras2/lib/python3.9/site-packages/keras/engine/training.py\", line 2123, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/lennon/miniconda3/envs/keras2/lib/python3.9/site-packages/keras/engine/training.py\", line 2111, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/home/lennon/miniconda3/envs/keras2/lib/python3.9/site-packages/keras/engine/training.py\", line 2079, in predict_step\n        return self(x, training=False)\n    File \"/home/lennon/miniconda3/envs/keras2/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/lennon/miniconda3/envs/keras2/lib/python3.9/site-packages/keras/engine/input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"functional_3\" is incompatible with the layer: expected shape=(None, 802, 80), found shape=(None, 802, 128)\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[42], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mload_model(m)\n\u001b[1;32m      7\u001b[0m ss, _ \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mread(test_audio)\n\u001b[0;32m----> 8\u001b[0m oop \u001b[38;5;241m=\u001b[39m \u001b[43mmk_preds_fa\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_audio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m p_smooth \u001b[38;5;241m=\u001b[39m smooth_output(oop\u001b[38;5;241m.\u001b[39mT, min_speech\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.3\u001b[39m, min_music\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3.4\u001b[39m, max_silence_speech\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.4\u001b[39m, max_silence_music\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m)\n\u001b[1;32m     11\u001b[0m p_smooth \u001b[38;5;241m=\u001b[39m p_smooth\u001b[38;5;241m.\u001b[39mT\n",
            "Cell \u001b[0;32mIn[22], line 166\u001b[0m, in \u001b[0;36mmk_preds_fa\u001b[0;34m(audio_path, hop_size, discard, win_length, sampling_rate)\u001b[0m\n\u001b[1;32m    163\u001b[0m   M \u001b[38;5;241m=\u001b[39m mss\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    164\u001b[0m   mss_in[i, :, :] \u001b[38;5;241m=\u001b[39m M\n\u001b[0;32m--> 166\u001b[0m preds \u001b[38;5;241m=\u001b[39m (\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmss_in\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m))\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    168\u001b[0m preds_mid \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcopy(preds[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m100\u001b[39m:\u001b[38;5;241m702\u001b[39m, :])\n\u001b[1;32m    170\u001b[0m preds_mid_2 \u001b[38;5;241m=\u001b[39m preds_mid\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
            "File \u001b[0;32m~/miniconda3/envs/keras2/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[0;32m/tmp/__autograph_generated_filemti8l65g.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/lennon/miniconda3/envs/keras2/lib/python3.9/site-packages/keras/engine/training.py\", line 2137, in predict_function  *\n        return step_function(self, iterator)\n    File \"/home/lennon/miniconda3/envs/keras2/lib/python3.9/site-packages/keras/engine/training.py\", line 2123, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/lennon/miniconda3/envs/keras2/lib/python3.9/site-packages/keras/engine/training.py\", line 2111, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/home/lennon/miniconda3/envs/keras2/lib/python3.9/site-packages/keras/engine/training.py\", line 2079, in predict_step\n        return self(x, training=False)\n    File \"/home/lennon/miniconda3/envs/keras2/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/lennon/miniconda3/envs/keras2/lib/python3.9/site-packages/keras/engine/input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"functional_3\" is incompatible with the layer: expected shape=(None, 802, 80), found shape=(None, 802, 128)\n"
          ]
        }
      ],
      "source": [
        "m = 'model d-DS.h5'\n",
        "\n",
        "#eval_path = \"/content/drive/My Drive/ICASSP 2021/OpenBMAT/eval-2\"\n",
        "  \n",
        "model = tf.keras.models.load_model(m)\n",
        "\n",
        "ss, _ = sf.read(test_audio)\n",
        "oop = mk_preds_fa(test_audio)\n",
        "\n",
        "p_smooth = smooth_output(oop.T, min_speech=1.3, min_music=3.4, max_silence_speech=0.4, max_silence_music=0.6)\n",
        "p_smooth = p_smooth.T\n",
        "see = preds_to_se(p_smooth, audio_clip_length=ss.shape[0]/22050.0)\n",
        "\n",
        "n_label = args.output_path\n",
        "\n",
        "with open(n_label, 'w') as fp:\n",
        "  fp.write('\\n'.join('{},{},{}'.format(round(x[0], 5), round(x[1], 5), x[2]) for x in see))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNar9zWbsAkSXk/T4nwwCjv",
      "collapsed_sections": [],
      "include_colab_link": true,
      "mount_file_id": "13dlCSNvlZ5PqBtUcTp30Iw42-Ipt7Vjp",
      "name": "doMusicAndSpeechDetection.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
